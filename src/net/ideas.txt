# Read & Control a remote instance; process event/signal data from it.
#     > PHASE 1: control a remote MagPi instance running on networked hardware.
#           > MANDATORY: RESOLUTION ON REMOTE INSTANCES, SUBDOMAINS AND PORT CONFLICTS.
#               -- Finding remote instance on a private network
#           > MANDATORY: DOCKER IMAGE AND INSTALLATION ON REMOTE HARDWARE FOR DEV/TEST.

#           > this needs a UI, but don't build a new one (make a NET ViewContainer panel).
#               > NET panel shows (a table of local and connected hosts by hostname/ip address + mac
#               > NET panel has(start, stop, restart buttons) controls connected hosts modules
#                   > start, stop & restart module methods refactored to consistent method on ViewController.
#                   > start, stop & restart module methods access control.
#                   > other ways for user interaction besides module UIs???

#           > is there a way to connect a '0W' to a laptop w/o a network router?
#           > location of a remote host: NEW symbology in the existing MAP module
#           > 'Swagger' integration for hosts?

# PHASE 2

# Finding a remote instance on a public network:
#         what data can be passed, and how?
#         > data sent via control_port differentiated from module REST data.
#         > streamed?
#         > format? binary, text, JSON???
#         when is data passed? incrementally, immediately or in batches? scheduled?
#         differentiating 'discovered' connected hosts. protocol for ident, capabilities, etc.
#       'challenge'/'response' protocol.
#           -- ping port [8080] and look for headers and correct response on a 'control_port'
#

#     > Galatea: read and interpret the Galatea sensor group
#           > (3DOF, temperature, see 'galatea_hdwe_ex')
#           > need configurable widget set for data (ability to visualize realtime data, control subsystems)
#               > pull code from 'sensor' projects ('galatea_hdwe_ex')
#               > special UI widgets for sensors, visualization?

# ....3
#     > BOX: remote install & control 'BOX' hardware (perhaps use Ansible?)
#           > GIVEN DEPS THIS POSSIBLE WITH ANSIBLE, OR K8S/DOCKER?

# publish/subscribe to internal module events:

#   find these codepoints in each module
#     ARX: audio signal detected, labels
#     CAM: image capture, motion detected, labels.
#     GPS: location updated (HARWARE).
#     MAP: None
#     MOT: motion detected, labels?
#     SDR: signal detected, lost.
#     TRX: signal detected.
#     WIFI: signal detected, lost. test passed, failed.
#
#          when trilaterating a signal, have the ability
#          to communicate with the remote instance so as
#          to receive accurate and timely signalpoint
#          updates for a given signal.


# SECURITY: ENCRYPT DATA SENT ACROSS NETWORK.
# authentication: password, token, mac address
# authorization: users, groups, modules

# see "securing flask blueprints..."




# Install Docker:
#   docker network create elastic
#   docker pull docker.elastic.co/elasticsearch/elasticsearch:8.17.2
#   docker pull docker.elastic.co/kibana/kibana:8.17.2

# docker run --name es01 --net elastic -p 9200:9200 -it -m 1GB docker.elastic.co/elasticsearch/elasticsearch:8.17.2
# docker run --name kib01 --net elastic -p 5601:5601 docker.elastic.co/kibana/kibana:8.17.2

# https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# ✅ Elasticsearch security features have been automatically configured!
# ✅ Authentication is enabled and cluster connections are encrypted.
#
# ℹ️  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
#   L**_NQ*00Wbbpx24wWqN
#
# ℹ️  HTTP CA certificate SHA-256 fingerprint:
#   9022782a7780c081f54194ce37726141fa7f77157d4b7a42565037aceda242d7
#
# ℹ️  Configure Kibana to use this cluster:
# • Run Kibana and click the configuration link in the terminal when Kibana starts.
# • Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
#   eyJ2ZXIiOiI4LjE0LjAiLCJhZHIiOlsiMTcyLjE4LjAuMjo5MjAwIl0sImZnciI6IjkwMjI3ODJhNzc4MGMwODFmNTQxOTRjZTM3NzI2MTQxZmE3Zjc3MTU3ZDRiN2E0MjU2NTAzN2FjZWRhMjQyZDciLCJrZXkiOiI2MGMtZ3BVQm9CcDdGcUc3aWlVTzpNS3ZtU0I5eFJBMjZhQk0xMWUtY0t3In0=
#
# ℹ️ Configure other nodes to join this cluster:
# • Copy the following enrollment token and start new Elasticsearch nodes with `bin/elasticsearch --enrollment-token <token>` (valid for the next 30 minutes):
#   eyJ2ZXIiOiI4LjE0LjAiLCJhZHIiOlsiMTcyLjE4LjAuMjo5MjAwIl0sImZnciI6IjkwMjI3ODJhNzc4MGMwODFmNTQxOTRjZTM3NzI2MTQxZmE3Zjc3MTU3ZDRiN2E0MjU2NTAzN2FjZWRhMjQyZDciLCJrZXkiOiI3VWMtZ3BVQm9CcDdGcUc3aWlYNzppX1pQR0lYS1I4LWNNY0pCbk9rZ0tRIn0=
#
#   If you're running in Docker, copy the enrollment token and run:
#   `docker run -e "ENROLLMENT_TOKEN=<token>" docker.elastic.co/elasticsearch/elasticsearch:8.17.2`
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# regenerate credentials
#   docker exec -it es01 /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic
#   docker exec -it es01 /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana

# We recommend storing the elastic password as an environment variable in your shell. Example:
#   export ELASTIC_PASSWORD="L**_NQ*00Wbbpx24wWqN"

# https://www.elastic.co/guide/en/elasticsearch/reference/current/security-basic-setup.html
# https://www.elastic.co/guide/en/elasticsearch/reference/current/security-basic-setup-https.html

# copy cert to local machine:
# docker cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .

# https://www.elastic.co/guide/en/enterprise-search-clients/python/current/connecting.html#connect-self-hosted
# https://www.elastic.co/guide/en/kibana/current/asset-tracking-tutorial.html



# valid ways of moving the data:
# - do ETL on JSON: extract worker and signals in MapAggregator component; push to Elastic.  [BLOCKING?]
#
# + Have the workers DUMP THE DATA ON EXIT (worker.stop() dumps only tracked items)); THEN push to Elastic.

#       DECORATE append_to_outfile in wifi_utils. have this be the elastic integration point.
#       [NOCARE, ONLY TRACKED ITEMS (+), includes 4/6 (not files) and solves 'when?']
#       indexes can be managed atomically. we can programmatically expunge signals and keep workers.

# - signals and workers use methods to make client calls in real-time; push to Elastic. [BLOCKING?, maintainence]
# - tight coupling to tracked objects; push to Elastic. [WHEN PUSHED? WHAT IS USEFUL ABOUT NON-TRACKED ITEMS?]

# > use a logstash http_polling connector and a mapping; poll module endpoints; pull to Elastic.
# - import files; pull to Elastic.
#
# A. IS THE CODE BLOCKING?
# B. What if Elastic is 'offline'; how does this affect the following use cases?
#       Module users, real-time (signals): not affected unless code is blocking!
#       Apparatus Users, real-time (map+signals)
#           OFFLINE  ANYWAY:
#               Ingest processes, continuity of data. availability, stability of components, decoupling
#               Kibana dashboards are offline, but not affecting module/apparatus users...
#               ML predictions, offline analysis
#       Maintainability
#
# Which is fastest and easiest to implement, so we can discover mistakes fast and correct them?
#
# ETC, Which offers the most downstream flexibility if needed to change.
# {"message": "", "48:9B:D5:F7:E2:C0": {"SSID": "CCOB_Library", "BSSID": "48:9B:D5:F7:E2:C0", "created": "13:47:44", "updated": "14:40:37", "elapsed": "00:52:05", "Vendor": "Extreme Networks Headquarters", "Channel": 1, "Frequency": 1281, "Signal": -99, "Quality": 24, "Encryption": false, "is_mute": false, "tracked": true, "signal_cache": [{"datetime": "2025-03-04 14:40:13.226561", "id": "54827a9e-49fc-482b-a46c-97f8262dccff", "lon": -105.068295, "lat": 39.916938, "sgnl": -99}, {"datetime": "2025-03-04 14:40:17.974622", "id": "f0a9b88b-78a2-48f3-ba16-708ffe507ad3", "lon": -105.068486, "lat": 39.916797, "sgnl": -99}, {"datetime": "2025-03-04 14:40:22.764689", "id": "1176761b-9e3c-434b-8e8d-58c1be094650", "lon": -105.068668, "lat": 39.916763, "sgnl": -99}, {"datetime": "2025-03-04 14:40:32.639993", "id": "573c5000-e9c6-4a05-87b9-41305449f3b1", "lon": -105.06867, "lat": 39.916891, "sgnl": -99}, {"datetime": "2025-03-04 14:40:37.499255", "id": "72dcbdbf-8d6c-4d20-9ac2-da809864e0c8", "lon": -105.068617, "lat": 39.916906, "sgnl": -99}], "tests": []}}

# if it doesn't exist, create a single index for all workers


